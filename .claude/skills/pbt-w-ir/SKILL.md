---
name: pbt-ir
description: Analyze a library function's documentation to produce a structured IR (JSON) and a properties_to_test.md. The IR is the quality gate before test generation — it classifies what is testable, what is ambiguous, and what is broken. Always produces a JSON, even if logical errors are found.
---

# Property-Based Testing IR Generator

Produce a structured IR JSON and a properties markdown file for a library function. The IR is the input to a downstream test generation agent. Always produce the JSON — even if there are logical errors, the record is valuable for later analysis.
your goal is not writing your own test cases, but producing a IR for downstream test generation agent.
you are allow to read and fetch related documentation, but do not writing your own experiment test cases.
**Outputs**:
- `ir/<library>/<function>.json`
- `ir/<library>/<function>_properties.md` (generated by `generate_properties_md.py`)

---

## Step 1 — Fetch Documentation (Optional)

Skip this step if the user provides documentation directly.

If not provided, fetch the full docstring, signature, and version using `help()` or a web fetch of the official documentation page. Do not load module notes.

```bash
uv run python -c "import pandas as pd; help(pd.DataFrame.merge)"
```

---

## Step 2 — Logical Errors and Ambiguities (Always First)

Read the documentation and classify issues before writing anything else. **Always continue and produce a JSON** — logical errors and ambiguities are recorded in the IR, not treated as blockers.

For every logical error and ambiguity, record its **source**: state whether it was found in the user-provided documentation, the fetched online documentation, or the function's own docstring. Use a `source` field on the error/ambiguity entry.

**Logical errors** — documentation contradicts itself, states something demonstrably false, or has contradictory parameter descriptions. Record each error with its source. Note in the JSON whether it is severe enough to make the specification untrustworthy. Properties derived from unsound parts of the spec are excluded.

**Ambiguities** — behavior that is underspecified. Do not search GitHub issues, changelogs, or any external resource to clarify ambiguities. Record each ambiguity with its source and place it directly in `unresolved_ambiguities`. Exclude the ambiguous parameter or behavior from the input domain and from properties.

---

## Step 3 — Input Domain

Define valid and invalid inputs. This becomes `assume()` calls and `@given` strategies in the generated tests.

The input domain is the universal precondition for all properties. A property's `when` field captures any narrower condition beyond this.

```json
"input_domain": {
  "right": "DataFrame or named Series",
  "how": "one of: 'left', 'right', 'inner', 'outer', 'cross'",
  "on": "str or list[str], must exist in both frames; None means join on index",
  "constraints": [
    "on and left_on/right_on are mutually exclusive"
  ],
  "invalid_inputs": [
    "how='cross' with on specified → raises MergeError",
    "on column not present in both frames → raises KeyError"
  ]
}
```

Omit parameters that are unresolved ambiguities or affected by logical errors.

---

## Step 4 — Properties to Test

Extract claims and specify each one formally. Only include properties derived from parts of the documentation that are sound — exclude anything that overlaps with a logical error or an unresolved ambiguity.

Each property has a **type** (where the claim comes from) and a **strategy** (what inputs to use and what to look for).

### Property Types

| type | meaning |
|---|---|
| `explicit` | directly and clearly stated in the docstring |
| `indirect` | stated but requires interpretation of phrasing or examples |
| `implicit` | logically follows from the spec but not stated outright |

### Strategy Field

The strategy specifies:
- **what inputs** to generate (valid, invalid, edge cases, specific parameter combinations)
- **what to look for** (assertion shape, comparison, exception type)

This is not Hypothesis code — it is a precise English description that tells the test-writing agent what to do.

Don't try to look for specific error messages.

### Property Schema

```json
{
  "id": "P<n>",
  "type": "explicit | indirect | implicit",
  "claim": "close paraphrase of the doc claim, or inferred claim",
  "expression": "Python expression fragment usable as assertion skeleton",
  "when": "narrower precondition beyond the input domain, or null",
  "strategy": "what inputs to generate and what behavior to check for",
  "source": "which part of the documentation this claim comes from"
}
```

### Example Properties

```json
"properties": [
  {
    "id": "P1",
    "type": "explicit",
    "claim": "inner join result contains only rows where key appears in both frames",
    "expression": "set(result[on]) <= set(left[on]) & set(right[on])",
    "when": "how='inner', on is a single column present in both frames",
    "strategy": "generate two DataFrames with a shared column of mixed overlapping and non-overlapping string keys. Assert result keys are the intersection.",
    "source": "docstring Returns section"
  },
  {
    "id": "P2",
    "type": "explicit",
    "claim": "invalid value for how raises an error", # bad example
    "expression": "pytest.raises(ValueError): df.merge(right, how=<invalid>)",
    "when": null,
    "strategy": "pass arbitrary strings not in the valid set, integers, None, and lists. Each should raise. Check that valid values never raise.",
    "source": "docstring Parameters: how"
  },
  {
    "id": "P3",
    "type": "implicit",
    "claim": "result column count equals sum of unique columns from both frames minus join key (appears once)",
    "expression": "len(result.columns) == len(set(left.columns) | set(right.columns))",
    "when": "no overlapping non-key columns, so no suffix needed",
    "strategy": "generate DataFrames with disjoint column sets except the key. Count columns in result and compare to expected.",
    "source": null
  }
]
```

---

## Step 5 — Unresolved Ambiguities

List all ambiguities here. Do not invent properties for these. Each entry must record its source.

```json
"unresolved_ambiguities": [
  {
    "id": "A1",
    "description": "behavior when both 'on' and 'left_on' are passed simultaneously",
    "source": "fetched online documentation — Parameters section does not specify precedence",
    "maintainer_note": "merge() does not document behavior when both 'on' and 'left_on' are provided."
  }
]
```

---

## Step 6 — Generate the Markdown

```bash
uv run python .claude/skills/pbt-w-ir/scripts/generate_properties_md.py ir/<library>/<function>.json
# outputs: ir/<library>/<function>_properties.md
```

---

## Full IR Schema

```json
{
  "metadata": {
    "function": "pd.DataFrame.merge",
    "library": "pandas",
    "version": "2.1.0",
    "signature": "merge(right, how='inner', on=None, ...)"
  },
  "logical_errors": [
    {
      "id": "E<n>",
      "description": "what is wrong",
      "severity": "severe | minor",
      "source": "user-provided doc | fetched docstring | fetched online documentation",
      "affects_properties": ["P<n>"]
    }
  ],
  "input_domain": {
    "<param>": "description",
    "constraints": [],
    "invalid_inputs": []
  },
  "properties": [
    {
      "id": "P<n>",
      "type": "explicit | indirect | implicit",
      "claim": "close paraphrase of the claim",
      "expression": "Python expression fragment",
      "when": "narrower precondition or null",
      "strategy": "what inputs to generate and what to check for",
      "source": "which part of the documentation this claim comes from"
    }
  ],
  "unresolved_ambiguities": [
    {
      "id": "A<n>",
      "description": "what is unclear",
      "source": "user-provided doc | fetched docstring | fetched online documentation",
      "maintainer_note": "concise developer-readable gap description"
    }
  ]
}
```
